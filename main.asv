%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%         MAIN SCRIPT FOR TRAINING AN AGENT TO INTERCEPT A TARGET         %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The following script loads auxiliary functions associated with the
% primary learning algorithm. It uses these and other functions to load
% pre-stored parameters values and build data structures that specify the
% properties of the spatial environment, training protocol, and agent.
%
% As specified below, the follow data structures must be constructed before
% running the learning algorithm: 
%       arena:      properties of the spatial environment
%       belief:     properties of the agent's belief about the environment
%       sampler:    properties that specify how the agent samples new anchor
%                       points to guide trajectories through the environment
%       planner:    properties that specity how the agent plans a trajectory 
%                       through the set of sampled anchor points    
%       trial:      properties that specify the training protocol, including
%                       target and obstacle locations
% 
% Once the data structures above have been constructed, a single agent can
% be generated by running:
%       res = runLearning(arena,belief,sampler,planner,trial);
%
% The output structure 'res' contains the results from the training
% protocol, including beliefs, planned and executed trajectories, and 
% outcomes of executed trajectories.
%
% For more info on any of the functions in the repo, run:
%       help myfunction


%% load auxiliary and plotting functions

addpath("auxiliaryFunctions/","auxiliaryFunctions/peaks2/","plottingFunctions/");


%% generate basic structures for environment, trial protocol, and agent

% default parameter values are specified in the functions 'loadPlotParams',
% 'loadEnvironmentParams','loadAgentParams', and 'loadTrialParams'.

% load plotting parameters
%   current options: 'white'
plotType = 'white';
plotParams = loadPlotParams(plotType);


% NOTE: the following functions build upon one another, and must be called 
% in order.

% generate agent and environment
%   current options: 'default'
envType = 'default';
[arenaParams,targetParams,obstacleParams] = loadEnvironmentParams(envType);
[arena,belief,target,obstacle] = generateEnvironment(arenaParams,targetParams,obstacleParams);

% generate agent
%   current options: 'default'
agentType = 'default';
[belief,sampler,planner] = loadAgentParams(agentType,belief);

% generate trial protocol
%   current options: 'singleTarget', 'multiTarget', 'obstacle'
exptType = 'singleTarget';
trialParams = loadTrialParams(exptType);
trial = generateTrialStructure(arena,target,obstacle,planner,trialParams);


%% load or simulate a single agent and plot results from a single trial

agent = load('exampleAgent.mat');

trialID = 2;
plotSingleTrial(agent.simResults,agent.arena,agent.belief,agent.trial,trialID,plotParams);

% to simulate a new agent and returns results (in data structure 
% 'simResults'), run:
%   simResults = runLearning(arena,belief,sampler,planner,trial);
%   plotSingleTrial(simResults,arena,belief,trial,trialID,plotParams);

%% simulate many agents, store results, and plot results averaged across agents

nAgents    = 50;
rewardRate = [];
pReward    = [];

parfor i=1:nAgents
    disp(['running agent ',num2str(i)]);
    res = runLearning(arena,belief,sampler,planner,trial);
    rewardRate = [rewardRate, res.trajectory.rewards ];     
    pReward    = [pReward,    res.belief.probReward  ];
end



